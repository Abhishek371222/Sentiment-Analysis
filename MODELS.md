# ğŸ¤– Model Documentation

## Overview

This document provides detailed explanations of all machine learning and deep learning models implemented in the Stock Prediction project.

---

## ğŸ“‘ Table of Contents

1. [Time Series Models](#time-series-models)
2. [Sentiment Analysis Models](#sentiment-analysis-models)
3. [Technical Indicators](#technical-indicators)
4. [Ensemble Methods](#ensemble-methods)
5. [Performance Metrics](#performance-metrics)

---

## Time Series Models

### 1. ARIMA (AutoRegressive Integrated Moving Average)

**Mathematical Formula:**
$$ARIMA(p,d,q): \Delta^d y_t = \mu + \sum_{i=1}^{p} \phi_i y_{t-i} + \sum_{i=1}^{q} \theta_i \epsilon_{t-i}$$

**Components:**
- **p (AR):** Autoregressive order - dependency on past values
- **d (I):** Integration order - degree of differencing for stationarity
- **q (MA):** Moving Average order - dependency on past errors

**Implementation:**
```python
from statsmodels.tsa.arima_model import ARIMA

model = ARIMA(data, order=(3, 1, 3))
results = model.fit(disp=0)
forecast = results.get_forecast(steps=30)
```

**Advantages:**
- âœ… Interpretable parameters
- âœ… Fast computation
- âœ… Well-suited for stationary data
- âœ… No GPU required

**Disadvantages:**
- âŒ Assumes linear relationships
- âŒ Requires stationarity
- âŒ Poor with non-linear patterns
- âŒ Sensitive to outliers

**Best For:** Stable, non-trending data

**Status:** âœ… Fixed & Working (v2.0)

---

### 2. SARIMA (Seasonal ARIMA)

**Formula:**
$$SARIMA(p,d,q)(P,D,Q,s) = ARIMA(p,d,q) \times Seasonal(P,D,Q,s)$$

**Key Difference:** Includes seasonal components for quarterly/yearly patterns

**Implementation:**
```python
from statsmodels.tsa.statespace.sarimax import SARIMAX

model = SARIMAX(data, order=(1,1,1), seasonal_order=(1,1,1,12))
results = model.fit(disp=0)
```

**Advantages:**
- âœ… Captures seasonality
- âœ… Better for periodic data
- âœ… Handles yearly patterns

**Disadvantages:**
- âŒ More parameters to tune
- âŒ Computational overhead
- âŒ Risk of overfitting

**Best For:** Data with clear seasonal patterns (months, quarters)

**Status:** âœ… Working

---

### 3. Facebook Prophet

**Decomposition:**
$$y_t = g(t) + s(t) + h(t) + \epsilon_t$$

Where:
- **g(t):** Trend (piecewise linear or logistic)
- **s(t):** Seasonality (Fourier series)
- **h(t):** Holiday effects
- **Îµ_t:** Error term

**Implementation:**
```python
from prophet import Prophet

df = pd.DataFrame({'ds': dates, 'y': values})
model = Prophet()
model.fit(df)
future = model.make_future_dataframe(periods=30)
forecast = model.predict(future)
```

**Advantages:**
- âœ… Handles missing data
- âœ… Robust to outliers
- âœ… Holiday effects
- âœ… User-friendly

**Disadvantages:**
- âŒ Less flexible for complex patterns
- âŒ Slower training
- âŒ Black-box model

**Best For:** Business forecasting with holidays/events

**Status:** âœ… Fixed & Working (v2.0)

---

### 4. LSTM (Long Short-Term Memory)

**Architecture:**
```
Input â†’ [Forget Gate | Input Gate | Output Gate] â†’ Hidden State â†’ Output
```

**Mathematical Gates:**
```
Forget Gate:    f_t = Ïƒ(W_f Â· [h_{t-1}, x_t] + b_f)
Input Gate:     i_t = Ïƒ(W_i Â· [h_{t-1}, x_t] + b_i)
Candidate:      CÌƒ_t = tanh(W_C Â· [h_{t-1}, x_t] + b_C)
Cell State:     C_t = f_t âŠ™ C_{t-1} + i_t âŠ™ CÌƒ_t
Output Gate:    o_t = Ïƒ(W_o Â· [h_{t-1}, x_t] + b_o)
Hidden State:   h_t = o_t âŠ™ tanh(C_t)
```

**Implementation:**
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

model = Sequential([
    LSTM(50, activation='relu', input_shape=(timesteps, features)),
    Dense(25, activation='relu'),
    Dense(1)
])
model.compile(optimizer='adam', loss='mse')
model.fit(X_train, y_train, epochs=100, batch_size=32)
```

**Advantages:**
- âœ… Captures long-term dependencies
- âœ… Excellent on non-linear data
- âœ… Best accuracy (120 RMSE)
- âœ… Handles variable-length sequences

**Disadvantages:**
- âŒ Requires GPU for fast training
- âŒ Black-box (difficult interpretation)
- âŒ Prone to overfitting
- âŒ Needs large datasets

**Best For:** Complex non-linear patterns, high-frequency data

**Status:** âœ… Working

---

### 5. GRU (Gated Recurrent Unit)

**Simplified Gates:**
```
Reset Gate:    r_t = Ïƒ(W_r Â· [h_{t-1}, x_t])
Update Gate:   z_t = Ïƒ(W_z Â· [h_{t-1}, x_t])
Candidate:     hÌƒ_t = tanh(W Â· [r_t âŠ™ h_{t-1}, x_t])
Hidden State:  h_t = (1 - z_t) âŠ™ h_{t-1} + z_t âŠ™ hÌƒ_t
```

**Advantages:**
- âœ… Faster than LSTM (30% quicker)
- âœ… Similar accuracy
- âœ… Fewer parameters

**Disadvantages:**
- âŒ Less powerful than LSTM
- âŒ Still requires GPU

**Best For:** Real-time predictions, resource-constrained environments

**Status:** âœ… Working

---

### 6. RNN (Vanilla Recurrent Neural Network)

**Formula:**
$$h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$
$$y_t = W_{hy} h_t + b_y$$

**Advantages:**
- âœ… Simple architecture
- âœ… Fast training
- âœ… Minimal parameters

**Disadvantages:**
- âŒ Vanishing gradient problem
- âŒ Poor long-term memory
- âŒ Lower accuracy than LSTM/GRU

**Best For:** Short sequences, baseline models

**Status:** âœ… Working

---

## Sentiment Analysis Models

### 1. BERT (Bidirectional Encoder Representations from Transformers)

**Architecture:** Transformer with 12 layers, 768 hidden units, 12 attention heads

**Pre-training Objectives:**
1. **MLM (Masked Language Model):** Predict masked tokens
2. **NSP (Next Sentence Prediction):** Predict sentence order

**Implementation:**
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased", 
    num_labels=3  # Positive, Negative, Neutral
)

inputs = tokenizer("Great stock performance!", return_tensors="pt")
outputs = model(**inputs)
sentiment = torch.argmax(outputs.logits, dim=1)
```

**Performance:**
- **Accuracy:** 87%
- **F1-Score:** 0.85
- **Speed:** 2-3 sec per batch

**Advantages:**
- âœ… State-of-the-art performance
- âœ… Bidirectional context
- âœ… Transfer learning
- âœ… 85%+ accuracy

**Disadvantages:**
- âŒ Slow inference
- âŒ Large model (440 MB)
- âŒ Requires GPU for speed

**Best For:** Production sentiment analysis, complex text

**Status:** âœ… Working

---

### 2. Naive Bayes

**Formula:**
$$P(Sentiment|Text) = \frac{P(Text|Sentiment) \cdot P(Sentiment)}{P(Text)}$$

**Implementation:**
```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(texts)

clf = MultinomialNB()
clf.fit(X, y)
predictions = clf.predict(X_test)
```

**Performance:**
- **Accuracy:** 78%
- **Speed:** <1ms per prediction

**Advantages:**
- âœ… Fast & simple
- âœ… Low memory
- âœ… Interpretable

**Disadvantages:**
- âŒ Lower accuracy
- âŒ Assumes feature independence
- âŒ Struggles with context

**Best For:** Fast baseline, resource-limited systems

**Status:** âœ… Working

---

### 3. Support Vector Machine (SVM)

**Formula:**
$$f(x) = \text{sign}\left(\sum_{i=1}^n \alpha_i y_i K(x_i, x) + b\right)$$

**Kernels:** Linear, RBF (Radial Basis Function), Polynomial

**Implementation:**
```python
from sklearn.svm import SVC
from sklearn.feature_extraction.text import TfidfVectorizer

X = vectorizer.fit_transform(texts)
clf = SVC(kernel='rbf', C=1.0)
clf.fit(X, y)
```

**Performance:**
- **Accuracy:** 81%
- **Speed:** 50ms per batch

**Advantages:**
- âœ… Good accuracy (81%)
- âœ… Works with high dimensions
- âœ… Memory efficient

**Disadvantages:**
- âŒ Slower than Naive Bayes
- âŒ Hyperparameter tuning needed
- âŒ Binary classification focus

**Best For:** Balanced accuracy/speed, structured data

**Status:** âœ… Working

---

### 4. Logistic Regression

**Formula:**
$$P(y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + ... + \beta_n x_n)}}$$

**Implementation:**
```python
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(max_iter=1000)
clf.fit(X, y)
```

**Performance:**
- **Accuracy:** 79%
- **Speed:** <1ms

**Advantages:**
- âœ… Fast
- âœ… Interpretable coefficients
- âœ… Probabilistic output

**Disadvantages:**
- âŒ Assumes linear separability
- âŒ Lower accuracy

**Best For:** Linear relationships, baseline models

**Status:** âœ… Working

---

## Technical Indicators

### 1. Simple Moving Average (SMA)

**Formula:**
$$SMA_n = \frac{P_1 + P_2 + ... + P_n}{n}$$

**Signal:** Trend direction, support/resistance

**Implementation:**
```python
df['SMA_20'] = df['Close'].rolling(window=20).mean()
df['SMA_50'] = df['Close'].rolling(window=50).mean()
```

---

### 2. Moving Average Convergence Divergence (MACD)

**Formula:**
$$MACD = EMA_{12} - EMA_{26}$$
$$Signal = EMA_9(MACD)$$
$$Histogram = MACD - Signal$$

**Signals:**
- MACD > Signal: Bullish
- MACD < Signal: Bearish
- Histogram crossover: Momentum change

---

### 3. Relative Strength Index (RSI)

**Formula:**
$$RSI = 100 - \frac{100}{1 + RS}$$
$$RS = \frac{\text{Average Gain}}{\text{Average Loss}}$$

**Thresholds:**
- RSI > 70: Overbought (potential sell)
- RSI < 30: Oversold (potential buy)

---

### 4. Bollinger Bands

**Formula:**
$$Middle = SMA_{20}$$
$$Upper = SMA_{20} + 2 \times \sigma$$
$$Lower = SMA_{20} - 2 \times \sigma$$

**Signals:**
- Price > Upper: Overbought
- Price < Lower: Oversold
- Squeeze: Breakout coming

---

### 5. Rate of Change (ROC)

**Formula:**
$$ROC = \frac{Price_t - Price_{t-n}}{Price_{t-n}} \times 100$$

**Signal:** Momentum strength

---

## Ensemble Methods

### Hybrid Approach (Combined Models)

```python
# Ensemble prediction
lstm_pred = lstm_model.predict(X_test)
arima_pred = arima_model.forecast(steps=len(X_test))
prophet_pred = prophet_forecast['yhat'].values

# Weighted average
ensemble_pred = (0.5 * lstm_pred + 
                 0.3 * arima_pred + 
                 0.2 * prophet_pred)
```

**Advantages:**
- Reduces model risk
- Better generalization
- Captures different patterns

---

## Performance Metrics

### Regression Metrics

**Mean Squared Error (MSE):**
$$MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2$$

**Root Mean Squared Error (RMSE):**
$$RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}$$

**Mean Absolute Error (MAE):**
$$MAE = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|$$

**R-Squared:**
$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$$

---

### Classification Metrics

**Accuracy:**
$$Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$$

**Precision:**
$$Precision = \frac{TP}{TP + FP}$$

**Recall:**
$$Recall = \frac{TP}{TP + FN}$$

**F1-Score:**
$$F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$$

**Confusion Matrix:** TP, TN, FP, FN visualization

---

## Model Selection Guide

| Scenario | Recommended Model | Reason |
|----------|------------------|--------|
| Stationary data | ARIMA | Interpretable, fast |
| Seasonal patterns | SARIMA/Prophet | Handles seasonality |
| Complex non-linear | LSTM | Best accuracy |
| Real-time inference | GRU | Speed priority |
| Sentiment (text) | BERT | State-of-the-art |
| Baseline/fast | Naive Bayes | Speed priority |

---

**Status:** âœ… Updated December 30, 2025

For implementation examples, see respective `.ipynb` files in each folder.
